{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cores: 8\n",
      "using:  cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "\n",
    "import jax \n",
    "from jax import random\n",
    "from jax.lib import xla_bridge\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "print(f\"number of cores:\", jax.local_device_count())\n",
    "print(f\"using: \", xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2., dtype=float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def dot(v1, v2):\n",
    "    return jnp.dot(v1, v2)\n",
    "\n",
    "dot(jnp.array([1.,1.,1.]), jnp.array([1.,2., -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000000, 3), (10000000, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating two large list of vectors\n",
    "rng_key = random.PRNGKey(42)\n",
    "vs = random.normal(rng_key, (20_000_000, 3))\n",
    "\n",
    "v1s = vs[:10_000_000, :]\n",
    "v2s = vs[10_000_000:, :]\n",
    "v1s.shape, v2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_batched = jax.jit(jax.vmap(dot, in_axes=(0, 0)))\n",
    "x_vmap = dot_batched(v1s, v2s)\n",
    "x_vmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_parallel = jax.pmap(dot)\n",
    "# x_pmap = dot_parallel(v1s, v2s) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 1250000, 3), (8, 1250000, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1sp = v1s.reshape(8, v1s.shape[0]//8, v1s.shape[1])\n",
    "v2sp = v2s.reshape(8, v2s.shape[0]//8, v2s.shape[1])\n",
    "v1sp.shape, v2sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_pmap = dot_parallel(v1sp, v2sp)\n",
    "# x_pmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1250000) <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(10000000,)\n",
      "VMAP and PMAP have the same results: True\n"
     ]
    }
   ],
   "source": [
    "dot_parallel = jax.pmap(jax.vmap(dot))\n",
    "x_pmap = dot_parallel(v1sp, v2sp)\n",
    "print(x_pmap.shape, type(x_pmap))\n",
    "\n",
    "# reshaping back to the original shape\n",
    "x_pmap = x_pmap.reshape(x_pmap.shape[0]*x_pmap.shape[1])\n",
    "print(x_pmap.shape)\n",
    "print(\"VMAP and PMAP have the same results:\", jax.numpy.allclose(x_vmap, x_pmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziaee/anaconda3/envs/jax/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:1794: UserWarning: The jitted function dot includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Measuring the difference between vmap() and pmap()\n",
    "\n",
    "dot_v = jax.jit(jax.vmap(dot))\n",
    "\n",
    "dot_pji = jax.pmap(jax.vmap(jax.jit(dot)))\n",
    "dot_pjo = jax.jit(jax.pmap(jax.vmap(dot)))\n",
    "dot_vj = jax.vmap(jax.jit(dot))\n",
    "dot_pjo(v1sp, v2sp);\n",
    "dot_pji(v1sp, v2sp);\n",
    "dot_v(v1s, v2s);\n",
    "dot_vj(v1s, v2s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.7 ms ± 757 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "648 ms ± 22.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "123 ms ± 1.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "40.7 ms ± 269 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dot_v(v1s, v2s).block_until_ready()\n",
    "%timeit dot_pjo(v1sp, v2sp).block_until_ready()\n",
    "%timeit dot_pji(v1sp, v2sp).block_until_ready()\n",
    "%timeit dot_vj(v1s, v2s).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3) (8, 3)\n",
      "[-0.74235083  0.64188436 -4.86064789]\n",
      "[-0.74235083  0.64188436 -4.86064789]\n",
      "[-0.74235083  0.64188436 -4.86064789]\n"
     ]
    }
   ],
   "source": [
    "# Explicitly using the in_axes parameter\n",
    "vs = random.normal(rng_key, (16, 3))\n",
    "v1s = vs[:8, :]\n",
    "v2s = vs[8:, :]\n",
    "\n",
    "print(v1s.shape, v2s.shape)\n",
    "\n",
    "def dot(v1, v2):\n",
    "    return jnp.vdot(v1, v2)\n",
    "\n",
    "print(jax.pmap(dot, in_axes=(0, 0))(v1s, v2s)[:3])\n",
    "print(jax.pmap(dot, in_axes=(1, 0))(v1s.T, v2s)[:3])\n",
    "print(jax.pmap(dot, in_axes=(1, 1))(v1s.T, v2s.T)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 3), (3, 8))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the in_axes parameter for broadcasting\n",
    "def scaled_dot(v1, v2, koeff):\n",
    "    return koeff * jnp.vdot(v1, v2)\n",
    "\n",
    "v1s_ = v1s \n",
    "v2s_ = v2s.T \n",
    "k = 1.0\n",
    "v1s_.shape, v2s_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the last parameter of the function, the scaling coefficient, is replicated across all the devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.74235083  0.64188436 -4.86064789]\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_batched = jax.pmap(scaled_dot, in_axes=(0, 1, None))\n",
    "print(scaled_dot_batched(v1s_, v2s_, k)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.74235083,  0.64188436, -4.86064789], dtype=float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the in_axes parameter with a Python container\n",
    "def scaled_dot(data, koeff):\n",
    "    return koeff * jnp.vdot(data['a'], data['b'])\n",
    "\n",
    "scaled_dot_pmapped = jax.pmap(scaled_dot, in_axes=({'a':0, 'b':1}, None))\n",
    "scaled_dot_pmapped({'a':v1s_, 'b':v2s_}, k)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 3), (3, 8), Array(True, dtype=bool))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the out_axes parameter\n",
    "def scale(v, koeff):\n",
    "    return v * koeff\n",
    "\n",
    "scaled_pmapped = jax.pmap(scale, in_axes=(0, None), out_axes=1)\n",
    "res = scaled_pmapped(v1s, 2.0)\n",
    "v1s.shape, res.shape, jnp.allclose(res.T, v1s*2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 10000000), (3, 10000000))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Large array example\n",
    "vs = random.normal(rng_key, (20_000_000, 3))\n",
    "v1s = vs[:10_000_000, :].T \n",
    "v2s = vs[10_000000:, :].T\n",
    "v1s.shape, v2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 8, 1250000), (3, 8, 1250000))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_device = jax.device_count()\n",
    "v1sp = v1s.reshape(v1s.shape[0], n_device, v1s.shape[1]//n_device)\n",
    "v2sp = v2s.reshape(v2s.shape[0], n_device, v2s.shape[1]//n_device)\n",
    "v1sp.shape, v2sp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Asks vmap to use the second dimension to map over (vmap does not see the group dimension, so its second\n",
    "dimension is the vector dimension)  \n",
    "**B**: Asks pmap to use the second (group) dimension over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1250000)\n",
      "(10000000,)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dot_parallel = jax.pmap(\n",
    "    jax.vmap(dot, in_axes=(1, 1)),  # A\n",
    "    in_axes=(1, 1))                 # B  \n",
    "x_pmap = dot_parallel(v1sp, v2sp)\n",
    "print(x_pmap.shape)\n",
    "x_pmap = x_pmap.reshape(x_pmap.shape[0] * x_pmap.shape[1])\n",
    "print(x_pmap.shape)\n",
    "print(jax.numpy.allclose(x_vmap, x_pmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parallel operators](https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.03571429 0.07142857 0.10714286 0.14285714 0.17857143\n",
      " 0.21428571 0.25      ] 1.0\n"
     ]
    }
   ],
   "source": [
    "# using collective ops and axis_name, if n elements be larger that the number of devices-> error\n",
    "arr = jnp.array(range(8))\n",
    "norm = jax.pmap(lambda x: x/jax.lax.psum(x, axis_name='p'), axis_name='p')(arr)\n",
    "print(norm, jnp.sum(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 25)\n",
      "(8, 25) 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalization example for a larger array\n",
    "arr = jnp.array(range(200))\n",
    "arr = arr.reshape(n_device, arr.shape[0]//n_device)\n",
    "print(arr.shape)\n",
    "\n",
    "norm = jax.pmap(lambda x: x/jax.lax.psum(jnp.sum(x), axis_name='p'), axis_name='p')\n",
    "narr = norm(arr)\n",
    "print(narr.shape, jnp.sum(narr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 25) 3.9999999999999996\n",
      "1.0 1.0 1.0000000000000002 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Normalization by groups\n",
    "arr = jnp.array(range(200))\n",
    "arr = arr.reshape(n_device, arr.shape[0]//n_device)\n",
    "\n",
    "norm = jax.pmap(lambda x: x/jax.lax.psum(\n",
    "    jnp.sum(x),\n",
    "    axis_name='p',\n",
    "    axis_index_groups=[[0,1], [2,3], [4,5], [6,7]]),\n",
    "    axis_name='p')\n",
    "narr = norm(arr)\n",
    "print(narr.shape, jnp.sum(narr))\n",
    "print(jnp.sum(narr[:2]), jnp.sum(narr[2:4]), jnp.sum(narr[4:6]), jnp.sum(narr[6:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nested maps  \n",
    "mixing collective ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a mix of `pmap()` and `vmap()` with two different collective operations. one `pmax()` operation\n",
    "performed inside each array located on a separate device and another `pmax()` operation\n",
    "performed between different devices.  \n",
    "\n",
    "**A**: A function using two collective ops across different axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 25)\n",
      "(8, 25)\n"
     ]
    }
   ],
   "source": [
    "arr = jnp.array(range(200))\n",
    "arr = arr.reshape(n_device, arr.shape[0] // n_device)\n",
    "print(arr.shape)\n",
    "\n",
    "f = jax.pmap(\n",
    "    jax.vmap(\n",
    "        lambda x: jax.lax.pmax(x, axis_name=\"v\") / jax.lax.pmax(x, axis_name=\"p\"), # A\n",
    "        axis_name=\"v\",\n",
    "    ),\n",
    "    axis_name=\"p\",\n",
    ")\n",
    "print(f(arr).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the global normalization example from Listing 7.19 to get rid of the\n",
    "`jnp.sum()` call on each device by replacing it with a global sum including the batch axis\n",
    "(introduced by `vmap()`) as well. We do it the following way: we calculate a sum using a\n",
    "collective operation across both axes simultaneously `axis_name=('p','v')` into the `psum()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 25)\n",
      "(8, 25) 1.0\n"
     ]
    }
   ],
   "source": [
    "arr = jnp.array(range(200))\n",
    "arr = arr.reshape(n_device, arr.shape[0] // n_device)\n",
    "print(arr.shape)\n",
    "\n",
    "norm = jax.pmap(\n",
    "    jax.vmap(lambda x: x / jax.lax.psum(x, axis_name=(\"p\", \"v\")), \n",
    "             axis_name=\"v\"),\n",
    "    axis_name=\"p\",\n",
    ")\n",
    "narr = norm(arr)\n",
    "print(narr.shape, jnp.sum(narr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# nested pmap example, for small arrays\n",
    "\n",
    "arr = jnp.array(range(8)).reshape(2, 4)\n",
    "n = jax.pmap(jax.pmap(lambda x: x/jax.lax.psum(x, axis_name=('rows', 'cols')), axis_name='cols'), axis_name='rows')(arr)\n",
    "\n",
    "print(jnp.sum(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# nested pmap example, using decorator style \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.pmap, axis_name='rows')\n",
    "@partial(jax.pmap, axis_name='cols')\n",
    "def n(x):\n",
    "    return x/jax.lax.psum(x, axis_name=('rows', 'cols'))\n",
    "\n",
    "arr = jnp.array(range(8)).reshape(2, 4)\n",
    "print(jnp.sum(n(arr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
